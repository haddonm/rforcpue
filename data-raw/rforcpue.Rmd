---
title: "rforcpue"
author: "Malcolm Haddon"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Data Exploration and CPUE Standardization}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE)
options(knitr.kable.NA = "",
        knitr.table.format = "pandoc")

options("show.signif.stars"=FALSE,"stringsAsFactors"=FALSE,
        "max.print"=50000,"width"=240)

library(rforcpue)
library(rutilsMH)
library(knitr)

```



## Catch Effort and Data Exploration

Before proceeding to generating a statistical standardization of commercial catch rates, data exploration, perhaps through plotting up different variables and how they might change through the years, or relative to one another, can often be informative about changes in any fishery for a particular species. The __rforcpue__ R package, in combination with __rutilsMH__, includes an array of functions that should assist with such data exploration and standardization. If a species' fishery includes CPUE data then plots of the distribution of catches, effort, and CPUE (perhaps as log(CPUE)) can be helpful in the interpretation of such CPUE, especially if there is sufficient data to allow for CPUE standardization. __rforcpue__ includes numerous functions that can assist with CPUE standardization. All of these functions are described below with examples of their use. 

There should be no expectation that the functions described here to be used in the standardization of CPUE constitute anything like a complete treatment. This vignette only provides a very brief introduction or pointer to get people started in the very broad field of statistical standardization of CPUE. There are many aspects not considered (e.g. how or whether to treat zeros). This vignette remains a draft and if you find errors, omissions, or obscurities do please let me know (see DESCRIPTION for email address). In addition if you wish to reference this package you can obtain one by typing `citation("rforcpue")` into the console, which will give you the latest version. The developmental version is held in my github repository _www.github.com/haddonm/rforcpue_, from where it can be installed or cloned.

The help written for each function can be accessed by typing ?<function-name>.
For example `r ?addnorm`. In RStudio that will bring up a help page that provides details of the function and its syntax, plus a working example that can be copied into the console to illustrate the working of the particular function. More generally, if you scroll to the bottom of any help page at the bottom you will see a hyperlink 'Index' that will take you to a complete list of the help pages available within __cede__. At the top of that list are links to the package Description and to this vignette. Another way to access the vignette is to type _browseVignettes("cede")_ into the console.

## Data Exploration

The main data set included with __cede__ is called _sps_ and contains typical fisheries data from a scalefish fishery. It is there mainly to assist with learning the operation and use of the different functions. Generally it would be better to use your own data but if you consider the _sps_ data set you will gain an understanding of a typical format. The _sps_ dataset is modelled on real fisheries data but is greatly altered in all of its variables. This reflects an on-going issue that fisheries scientists need to deal with, which is the need for confidentiality. It would have been better had I simply included some real data for a known fishery. This could have been done but then details of location and of which vessel was doing the fishing for each record would need to be removed. As spatial information and the vessel fishing are very often highly influential, omitting such data would have defeated the primary purpose of including the data. Fortunately, the results of standardization can usually be displayed, but the raw data needs to be treated carefully. Similar considerations need to be given to each separate analysis to ensure that issues of confidentiality are not compromised. This is especially important when using the sketch mapping functions in __cede__. Such sketch maps can be very useful for detecting spatial heterogeneity but very often cannot be widely displayed. Nevertheless, such information should be considered even if not disseminated.  

```{r spscontents, echo=TRUE }

data(sps)
head(sps)
properties(sps)
```

The _properties_ function categorizes the contents of a data.frame, counting the number of NAs in each variable, if any, listing their class, their minimum and maximum (if applicable) and finally printing an example of the contents. I find this function quite useful when beginning to use a different data.frame. Generally I refer to variables within a data.frame by their names, as in, for example, `yrs = sort(unique(sps[,"Year"])`, so it is important to know if the names are capitalized or not, as well as knowing exactly which variables are present.

Once we have our data available for analysis it is often a good idea to find ways to summarize how they vary relative to one another. With fisheries data it is common to want to know how different factors influence the total catch or effort and whether these vary by year. Typically one might use the R function _tapply_ to conduct such examinations. I tend to use this function many times so to simplify its use one can use the _tapsum_ function from within __cede__.

For example, the seasonality of catches can be indicative of the typical behaviour of the fishery within a year.

```{r echo=TRUE}
round(tapsum(sps,"catch_kg","Year","Month"),2)
```

Then we might examine the catch (or effort, or etc, etc) by zone where the zones are in sequence along the coast (or they would be if this was a real fisheries data).

```{r echo=TRUE}
tapsum(sps,"catch_kg","Year","Zone")
```

We are not limited to summarizing catch but, for example could also look at the distribution of effort as total number of hours (note the change to the default value of div so that the total number of hours is not divided by 1000). By pointing the function call to a new object, in this case _effbyyr_, one can then plot the results.

```{r echo=TRUE}
effbyyr <- tapsum(sps,"Effort","Year","Zone",div=1.0)
effbyyr
```

```{r ploteffort, echo=TRUE, fig.width=7,fig.height=4.5,fig.align="center"}
# plotprep(width=7,height=4.5)
ymax <- max(effbyyr,na.rm=TRUE)
label <- colnames(effbyyr)
yrs <- as.numeric(rownames(effbyyr))
par(mfrow=c(1,1),mai=c(0.45,0.45,0.05,0.05)) 
par(cex=0.85, mgp=c(1.35,0.35,0), font.axis=7,font=7,font.lab=7) 
plot(yrs,effbyyr[,label[1]],type="l",lwd=2,col=1,ylim=c(0,ymax),
     ylab="Total Effort (Hours) by Zone per Year",xlab="",
     panel.first=grid())
lines(yrs,effbyyr[,label[2]],lwd=2,col=2)
lines(yrs,effbyyr[,label[3]],lwd=2,col=3)
# the "topright"" can also be "bottomright", "bottom", "bottomleft", "left", 
# # "topleft", "top", "right" and "center". Alternatively one can give an
# x and a y location, see ?legend, which is a function from graphics package
legend("topright",label,col=c(1,2,3),lwd=3,bty="n",cex=1.25)

```

__Figure 1.__ A plot of total effort by zone, showing that a visual illustration can often more easily highlight changes in a fishery's dynamics.



DayNight is another factor that can have large consequences for catches and catch rates. Check the description of the _sps_ data set using `?sps to see details concerning each field in the data.frame.

```{r echo=TRUE}
tapsum(sps,"catch_kg","Year","DayNight")
```

One of the most influential factors within each fishery is the vessel doing the catching. Often this is also a reflection of the skipper of the vessel as well as the relative performance of the boat itself. If both skipper and vessel are available then use both. Nevertheless, it is often the case the vessel name is the only information available about the vessel's fishing power relative to other vessels (or divers, etc) in the fleet. It is possible to pay special attention to catch-per-vessel, although the following analysis is more general than that and can be applied to, for example, catch-by-month relative to Depth Category.

```{r catchbyvessel, echo=TRUE}
cbv <- tapsum(sps,"catch_kg","Vessel","Year") # often more vessels than years
total <- rowSums(cbv,na.rm=TRUE)
cbv1 <- cbv[order(total),]   # sort by total catch smallest catches first
round(cbv1,2)

```

Obviously some vessels will be much more influential than others simply because they catch a great deal more than others and hence introduce many more records into the database. Similar things could be said about effort, but also the number of records present for a particular vessel will influence any subsequent analysis.


```{r yeasrbubble, fig.width=7,fig.height=5,fig.align="center"}
# plotprep(width=8,height=6) # not needed in the vignette
to <- turnover(cbv1)
yearBubble(cbv1,ylabel="sqrt(catch-per-vessel)",diam=0.125,txt=c(2,3,4,5),
           hline=TRUE)

```

__Figure 2.__ This hypothetical fishery is clearly dominated by four or five vessels with numerous minor players. Additionally, before 2007 there were a few more productive fishers present (this reflects the structural adjustment in the Commonwealth from which this simulated data derives). The optional horizontal lines merely delineate the individual vessels; leave out _hline_ for a cleaner plot with less ink. The top two rows of numbers is the total catch per year and the bottom row of numbers is the number of vessels reporting in each year.


It is likely that if the data from the bottom nine vessels were omitted there would be no effect on any results as their catches are so minor in a relative sense. It is clear those vessels are merely casual occurrences within the fishery. In a real fishery one might expect many more active vessels.

While the main vessels are reasonably consistent in terms of reporting from this fishery, other vessels came and went. To summarize such activity one can use the _turnover_ function which summarizes the year-to-year changes in which vessels report being active.

```{r echo = TRUE}
print(to)
```

The _Continue_ column lists how many vessels continued from the preceding year, the _Leave_ column designates how many left relative to the previous year, while the _Start_ column is literally how many started reporting in that year. The _Total_ is the total reporting in each year. No attempt is made to follow individual vessels, for which a separate function would need to be written. This distinction is necessary because some vessels start reporting and then only occasionally report from a fishery until they eventually stop reporting. If such vessels were classed as continuing across their initial year and final year, irrespective of whether they fished in every year, that would generate a somewhat different matrix of catch-by-vessel. Which analytical strategy to use would depend very much on what question is being asked.

### The Addition of CPUE data

You will have noticed that the data came with catch and effort but not CPUE, so we need to calculate that. In the following I test for the presence of zeros in the catch and effort to avoid generating errors of division and when taking logs (divide-by-zero errors will compromise the analysis). In fact, as the _properties_ call showed there were no _NA_ values, but it remains worth checking when dealing with real data. While we are adding CPUE we can also group the depth data into depth classes to provide that option when standardizing the CPUE data. We will include a brief discussion of continuous versus categorical factors later on.

```{r includeCPUE, echo=TRUE}
sps$CE <- NA     # make explicit space in the data.frame
sps$LnCE <- NA
pick <- which((sps$catch_kg > 0) & (sps$Effort > 0))
sps$CE[pick] <- sps$catch_kg[pick]/sps$Effort[pick]
sps$LnCE[pick] <- log(sps$CE[pick])   # natural log-transformation
# categorize Depth
range(sps$Depth,na.tm=TRUE)  # to aid selection of depth class width
sps$DepCat <- NA
sps$DepCat <- trunc(sps$Depth/25) * 25
table(sps$DepCat)

```


It is clear from the table of records by depth that most of the fishing occurs in waters of 150 meters or less.

Tables of numbers are very informative but sometimes it is much easier to gain a visual impression of patterns in one's data by plotting them. Typically, with fisheries data, one might plot each variable, such as catch, effort, log(CPUE), depth, etc, by year to see whether changes have occurred through time. Such changes might affect any analysis applied so it is always a good idea to examine (explore) one's data before using it. __cede__ provides a function _histyear_ that can plot a histogram of a selected variable by year. Use `r ?histyear` to see details of this function. 

```{r cebyyr, echo=TRUE, fig.width=7,fig.height=6,fig.align="center"}

outH <- histyear(sps,Lbound=-1.75,Rbound=8.5,inc=0.25,pickvar="LnCE",
                 years="Year",varlabel="log(CPUE)",plots=c(4,3))

```

__Figure 3.__ The distribution of the log(CPUE) each year for which data is available. The green lines are fitted normal distributions put there purely for reference (log-transformation should normalize log-normal data, but note the spikiness in some of the plots).


```{r depthbyyr, echo=TRUE, fig.width=7,fig.height=6,fig.align="center"}

outH <- histyear(sps,Lbound=0,Rbound=375,inc=12.5,pickvar="Depth",
                 years="Year",varlabel="Depth (m)",plots=c(4,3),vline=120)

```

__Figure 4.__ The distribution of reported mean depth of fishing each year. The green lines are fitted normal distributions there for reference, the blue lines, from the vline parameter are merely reference lines to ease comparisons between years.



```{r depthbyyr2, echo=TRUE, fig.width=7,fig.height=6,fig.align="center"}

outH <- histyear(sps,Lbound=0,Rbound=10,inc=0.25,pickvar="Effort",
                 years="Year",varlabel="Effort ('000s Hrs)",plots=c(4,3),
                 vline=NA)

```

__Figure 5.__ The distribution of reported Effort each year. The green lines are fitted normal distributions there for reference. Note the spikes of reporting at four hours.


Spikes can be seen in each of the graphs and the question needs to arise whether this is due to rounding by the fishers or is a real phenomenon. In fact, unless dealing with counts of fish caught (quite possible in some fisheries) then rounding invariably occurs when estimating catches but also in effort.

```{r catchvseffort, fig.width=7,fig.height=5,fig.align="center"}
par(mfrow=c(1,1),mai=c(0.45,0.45,0.05,0.05)) 
par(cex=0.85, mgp=c(1.35,0.35,0), font.axis=7,font=7,font.lab=7)  
plot(sps$Effort,sps$catch_kg,type="p",pch=16,col=rgb(1,0,0,1/5),
     ylim=c(0,500),xlab="Effort (Hrs)",ylab="Catch (Kg)")
abline(h=0.0,col="grey")

```

__Figure 6.__ A plot of catch against effort for each record in the _sps_ data.frame. The catch axis has been truncated at 500 kg so as to allow the rounding of catches to be less compressed and more visually obvious. It should be clear there is rounding at every half hour between 2 - 6 hours. In addition, there is rounding at about 30 kg steps from 30 - 300 kg, with other categories above that. The 30-33kg rounding reflects a belief that a standard fish bin contains about 30-33Kg of fish.


The uneven grid like nature of the catch and effort data is reflected in the CPUE data, which might (ought to?) make one skeptical about the notion of a statistical model attempting to predict such values. While the residual errors between the predicted and observed cpue values (which is what fitting the statistical model is trying to minimize) might be described by a less granular statistical distribution they do derive from a comparison of smooth predicted values with the grouped observed values, so any results are likely to be uncertain and to under-estimate any inherent variation. The presence of such rounding behaviour in the reporting also implies that arguing about exactly which statistical distribution to use to provide a description of the residual distribution is often not useful. Consistent treatment through time may be more valuable for communicating results, but if alternatives are suggested then a simple solution is to present the results from each alternative for comparison.

Despite such problems it is possible to derive useful information from fisheries data. It is generally recognized that fisheries data in general is noisy and potentially contains many errors, especially when considering the less important species that fall into the data-poor category. Nevertheless, the challenge remains one of attempting to obtain useful and usable information from analyzing such data.

### Plotting Sketch Maps of Spatial Data

Since the advent of GPS and GPS plotters very many fishers use this equipment and fisheries departments have started to ask for precise location data accordingly. If such latitude and longitude data (or eastings and northings) are available it is often informative to plot such data as a sketch map to illustrate the focus and range of a fishery. __cede__ also provides the capacity to generate such sketch maps instead of using a full GIS. The idea here is not to conduct detailed spatial analyses, for which a GIS is better suited. Instead the idea is simply to gain a rapid impression of the operation of a fishery. Of course, care needs to be taken with such plots as they very obviously contain confidential information (such as exactly where fishers have been operating). This is especially important when there are very few fishers involved in a fishery. So while such images may not be able to be displayed in meetings they remain useful for data exploration purposes.

```{r sketchmap1, echo=TRUE, fig.width=7,fig.height=5.5,fig.align="center"}
leftlong <- 143.0;  rightlong <- 150.0
uplat <- -40.0;  downlat <- -44.6
plotaus(leftlong,rightlong,uplat,downlat,gridon=1.0)
addpoints(sps,intitle="Location of Catches")
plotLand(incol="blue")

```

__Figure 7.__ A sketch map of the the Lat Long data within the _sps_ data set. There are clearly a number of points reported to be out over what would be abyssal plain, but the majority of points define the range of the fishery.


Rather than show individual points it is also possible, by using the function _plotpolys_, to aggregate catches into different geographical sub-divisions (e.g. 0.25 or 0.5 degree squares, definable with the _gridon_ parameter). If these are coloured relative to the density of total catches the locations where most of the yield of a fishery derives from becomes apparent. The output, from the function includes the plotting but also the sub-divisions used and the counts of each of those sub-divisions. The final plotting of the land is merely to provide a tidy looking plot.


```{r sketchmap2, echo=TRUE, fig.width=7,fig.height=5.5,fig.align="center"}
leftlong <- 143.0;  rightlong <- 150.0
uplat <- -40.0;  downlat <- -44.6
plotaus(leftlong,rightlong,uplat,downlat,gridon=1.0)
plotpolys(sps,leftlong,rightlong,uplat,downlat,gridon=0.2,leg="left",
          intitle="0.2 degree squares",mincount=2,namecatch="catch_kg")
plotLand(incol="pink")

```

__Figure 8.__ A sketch map of the the Lat Long data within the _sps_ data set with catches aggregated into 0.2 degree squares. By requiring at least 2 records in each square before inclusion some of the deeper water extraneous records have been eliminated (although not all). The red, green, and royal blue squares denote the areas generating the greatest yields.


Such sketch maps can be helpful, especially when plotting a single year's data to illustrate how the extent of a fishery varies through time. There are obvious limitations. There is no formal map projection, one merely alters the width and height of the plot until the visual representation of the land looks acceptable. In addition there are islands missing so as to limit the size of the underlying coastal definition data set (to see this try entering _head(cede:::aus,30)_ into the console).

One is not limited to plotting up catches, by altering which variable is identified in the parameter _namecatch_ it is possible to plot up other variables but this aspect is still under development. Currently catches are assumed to be reported as kg, and these are divided by 1000 to focus on tonnes of catch. Modifying this scaling of the polygons is the issue with plotting say effort, although sometimes with effort dividing by 1000 is not an issue. Future versions of __cede__ should address this issue.

## CPUE Standardization

### Introduction

If one were to search online for CPUE standardization it would quickly become apparent that this is a very large subject with many alternative approaches and strategies. Here I will introduce two approaches, the first uses General and Generalized Linear Models (LMs and GLMs) and the second uses Generalized Additive Models (GAMS). This will only be a brief introduction to the subject but the hope is that such an introduction would enable users to explore further and develop approaches best suited to their own fisheries. 

Commercial catch and effort (CPUE) data are used in very many fishery stock assessments in Australia as an index of relative abundance. Using CPUE in this way assumes there is a direct relationship between catch rates and exploitable biomass. However, many other factors can influence catch rates, including vessel, gear(fishing method), depth, season (month), area, and time of fishing (e.g. day or night). The use of CPUE as an index of relative abundance requires the removal of the effects of variation due to changes in factors other than stock biomass, on the assumption that what remains will provide a better estimate of the underlying biomass dynamics. That the outcome provides a better approximation to the stock biomass trends remains an assumption because there may well be other factors influencing CPUE for which no data are available and thus cannot be included in any standardization. However, at least variation in CPUE due to the factors that are included is accounted for. The process of adjusting the time series of CPUE for the effects of other factors is known as standardization and the accepted way of doing this is to use some statistical modelling procedure that focuses attention onto the annual average catch rates adjusted for the variation in the averages brought about by all the other factors identified. Idiosyncrasies between species and methods across Australia means that each fishery/stock for which standardized catch rates are required entails its own set of conditions and selection of data. 

### The Limits of Standardization

The use of commercial CPUE as an index of the relative abundance of exploitable biomass can be misleading when there are factors that significantly influence CPUE but cannot be accounted for in a statistical standardization analysis. Over the last few decades the management of many Australian fisheries have undergone significant changes. For example, in the Commonwealth fisheries there was the introduction of the quota management system into the SESSF in 1992, and the introduction of the Harvest Strategy Policy (HSP) and associated structural adjustment in 2005 - 2007. The combination of limited quotas and the HSP is now controlling catches in such a way that many fishers have been altering their fishing behaviour to take into account the availability of quota and their own access to quota needed to land the species taken in the mixed species SESSF. In other jurisdictions the introduction of such things as extensive marine protected areas have altered the fishing behaviour of commercial fishers and this too can effectively break CPUE time-series, which adds complexity to any analysis.


## Methods

### Initial Data Selection

Fisheries data is often noisy and can contain obvious errors and mistakes (e.g. an inshore species reportedly being caught in 6000 m of water). The data exploration mentioned earlier should allow one to defensibly select data for further analysis (i.e. defensibly remove implausible data points). Often such data selection is aimed at identifying records that represent typical activities in each fishery concerned. In particular some selection criteria are aimed at focussing on records where the species is being targeted. For example, most species have a depth range within which they are typically caught. Ideally, an agreed depth range should be used so that it becomes standard to select data records between some minimum and maximum depth range. A second example relates to one vessel in the SESSF catching a particular species by a particular gear having catch rates 10 - 20 times those of other vessels fishing in the same places at the same time. Further exploration indicated that the vessel concerned had misunderstood how to fill in the log book so their data was removed from subsequent analysis. Whatever decisions are made about any selection or filtering of data, each choice should be defensible and it should be possible to present the evidence for the selection made (e.g. illustrate extreme values, typical depth ranges, unusual vessels). The ultimate defense for making a data selection is to present the analysis with and without the selection to demonstrate any effect. 

Once a defensible set of data records have been selected there are other modifications needed. At its most basic a linear model is very similar to a regression analysis. If you imagine conducting a regression of Log(CPUE) against Year so as to evaluate how those catch rates have changed through time then all that would come out would be a single line having two parameters, an intercept and gradient. There are only two parameters because it would treat the factor 'Year' as a continuous variable. What we actually want is a separate index for each year, we need to treat the 'Year' factor as a categorical factor rather than as a continuous variable. Thus, for example, 1986 - 2016 is not a series of numbers between those years, but rather represents separate years. Below we will illustrate the use of using all categorical factors and then a different illustration showing how to include a continuous variable such as depth, into a standardization.

### Standardization

The use of _properties_ indicates that the _sps_ data set contains six clear factors: Year, Month, Vessel, Depth or DepCat, DayNight, and Zone. The Zone factor is a subdivision of mainly the Latitude factor although longitude is also in there to a lesser extent.

First we need to convert some of the factors into categorical factors using the eponymous function _makecategorical_. It is good practice not to over-write your original data.frame so here the _sps_ name is slightly modified to _sps1_.  

```{r makecategorical, echo=TRUE}
properties(sps)
labelM <- c("Year","Zone","Vessel","Month","DayNight","DepCat")
sps1 <- makecategorical(labelM,sps)
properties(sps1)

```


Note that after using _makecategorical_, the factors of interest within _sps1_ are now listed as factors rather than numeric, and that is enough to alter the analysis to something more like an anova than a regression analysis so that we obtain a parameter for each level of the factors used. 

The next step is to generate a statistical model to use in a standardization, and for that we use the function _makeonemodel_. This generates a model of class __formula__, which class is needed by all the statistical methods. If you use ?makeonemodel you will see reference to interaction terms and these will be discussed later on.

```{r makeamodel, echo=TRUE}
labelM <- c("Year","Zone","Vessel","Month","DayNight","DepCat")
sps1 <- makecategorical(labelM,sps)
mod <- makeonemodel(labelM)
mod
class(mod)
```

Each of the standardization methods we will use require that each statistical model to be examined needs to be a __formula__. If you enter _makeonemodel_, without brackets, into the R console you will see the final `form <- as.formula(form)`, which achieves this requirement.

If we are going to use a simple linear model then we can proceed using the function _dosingle_ (try ?dosingle or just dosingle). We point the output of this function to the _out_ object because there is an enormous amount of information generated, you can see this by using just _str(out)_.

```{r standardize, echo=TRUE}
labelM <- c("Year","Zone","Vessel","Month","DayNight","DepCat")
sps1 <- makecategorical(labelM,sps)
mod <- makeonemodel(labelM)
out <- dosingle(mod,sps1)   # no interaction terms this time
str(out,max.level=1)

```

One of the components of the _out_ object is the _optModel_, which, not surprisingly, represents the optimum model. It is possible to run the generic functions _summary_ and _anova_ on this component of the out object. The _summary_ function (`summary(out$optModel)`) will generate the parameters (on the log-scale) and a few other details. the _anova_ function determines the significance of each factor.

```{r ananova, echo=TRUE}

anova(out$optModel)
```

#### The Mean Year Estimates

For the lognormal model the expected back-transformed year effect involves a bias-correction to account for the log-normality; this then focuses on the mean of the distribution rather than the median:

$$CPU{{E}_{t}}={{e}^{\left( {{\gamma }_{t}}+\sigma _{t}^{2}/2 \right)}}$$

where ${\gamma }_{t}$ is the Year coefficient for year t and $\sigma _{t}$ is the standard deviation of the log transformed data (obtained from the analysis). The year coefficients were all divided by the average of all the Year coefficients to simplify the visual comparison of catch rate changes.

$$C{{E}_{t}}=\frac{CPU{{E}_{t}}}{\left( \sum{CPU{{E}_{t}}} \right)/n}$$

where CPUE~t~ is the yearly coefficients from the standardization, (CPUE~t~)/n is the arithmetic average of the yearly coefficients, n is the number of years of observations, and CE~t~ is the final time series of yearly index of relative abundance.

All of this can be obtained in two ways. Within the _out_ object there is the _Results_ matrix which contains both the geometric mean estimates along with the optimum statistical model. Similarly the _StErr_ object within _out_ contains the standard error estimates for each of the estimated time-series.

```{r out_results, echo=TRUE}
cbind(out$Results,out$StErr)
```

Alternatively, if all the details are wanted there is another function within __cede__ called _getfact_, which provides those extra details.

```{r getfact, echo=TRUE}

round(getfact(out$optModel,"Year"),5)
```

The standardizations provide parameters for each level of each factor except the first level in each case. These are all assumed to have a log-transformed value of 0.0 (= 1.0 on the nominal scale). All the other parameters (when log-normal errors are used, are proportional to the first level). Thus the LogCE column is the output from the standardization. The bias-adjusted transformation back to the nominal scale is described in the equations above.
The 'Scaled' column is the same as the 'Coeff' column except it is has been divided through by the mean of the series. This sets the average value to 1.0, which permits simple visual comparison with other time-series. The 'SE' column provides the basis for generating the log-normally distributed confidence intervals. 

```{r cpueplot, echo=TRUE, fig.width=7,fig.height=4.5,fig.align="center"}
# plotprep(width=7,height=4.5)
plotstand(out,bars=TRUE)

```

__Figure 9.__ The standardization of the cpue data within the _sps_ data set. The dashed line is the geometric mean CPUE while the solid line with 95% confidence intervals is the standardized CPUE. In a few places the difference between the standardized CPUE and the geometric mean CPUE is greater than the 95% log-normal confidence intervals.

One issue with this plot is that the scale makes little sense to many people, especially Industry members who are more used to the nominal scale at which they operate personally. Given that the average of both the geometric mean and the optimum model is 1.0, both can be multiplied by a constant to rescale the plots. If we calculate the geometric mean CPUE for the whole fishery we can use that as a multiplier and that will place each time-series on a recognizable nominal scale. This can be done using the function _geomean_ and include the _geo_ option of _plotstand_.


```{r cpueplot2, echo=TRUE, fig.width=7,fig.height=4.5,fig.align="center"}
# plotprep(width=7,height=4.5)
geom <- geomean(sps$CE)
plotstand(out,bars=TRUE,geo=geom)

```

__Figure 10.__ The standardization of the cpue data within the _sps_ data set. The dashed line is the geometric mean CPUE while the solid line with 95%. These trajectories both have an average of the overall geometric mean CPUE. 

### Diagnostic Plots for the Standardizations

Rather than consider only the numerical details of the relative fit of different models there are some classical diagnostic plots that can be produced that assist in the identification of issues with an analysis if any exist.

The most basic is built into R.

```{r diagnostic1, echo=TRUE, fig.width=7, fig.height=6}
par(mfrow=c(2,2),mai=c(0.45,0.45,0.05,0.05),oma=c(0.0,0,0.0,0.0)) 
par(cex=0.85, mgp=c(1.35,0.35,0), font.axis=7,font=7,font.lab=7) 
plot(out$optModel)

```

__Figure 11.__ Four diagnostic plots for the optimum statistical model calculated above. 


Within the four plots the one containing the _qqline_, which ideally should be a straight line, provides information concerning the normality of the distribution of residuals. One way of examining that in more detail


```{r diagnostic2, echo=TRUE, fig.width=7, fig.height=5}

par(mai=c(0.45,0.45,0.15,0.05),font.axis=7)
qqdiag(out$optModel,plotrug=TRUE,)


```

__Figure 12__ A more informative qqline plot that includes the 90% and 95% quantiles of the distribution of residuals. In this case the lower part of the distribution indicates that the lower 5% of points deviates from normality whereas the upper 95% adheres to normality (the straight line) remarkably well. The 'rug' on the bottom of the left hand plot indicates the density of the data points. The histogram illustrates the normality of the combined distribution (the green curve) as well as the same percentile points. 

__Figure 3__ illustrating the distribution of the log(CPUE) each year also indicates the relative normality of the log-transformed data each year. In the case shown in __Figure 12__ the deviation from normality is not major, however, the fact that occurs primarily with the smaller catch-rates suggests some factor acting on the smaller cpue values is not being accounted for very effectively.


### Sequential Development of Statistical Models

It is often helpful to examine the standardizations as they increase in complexity so that the relative influence of each factor becomes clearer. However, to do this would require a little more R code.

```{r contributions, echo=TRUE }
# first make a matrix to hold the results
labelM <- c("Year","Zone","DepCat","Vessel","Month","DayNight")
columns <- c("adjR2","incR2","RSS","MSS","Npar","nobs","AIC")
nummod <- length(labelM)
yrs <- sort(unique(sps$Year))
nyrs <- length(yrs)
results <- as.data.frame(matrix(0,nrow=nummod,ncol=length(columns),
                         dimnames=list(labelM,columns)))
trends <- as.data.frame(matrix(0,nrow=nyrs,ncol=length(labelM),
                         dimnames=list(yrs,labelM)))
for (i in 1:nummod) {  # sequentially build the models i=1
   mod <- makeonemodel(labelM[1:i])  # When i = 1 LnCE ~ Year
   out <- dosingle(mod,sps1)
   trends[,i] <- out$Results[,"optimum"]
   outsum <- summary(out$optModel)
   aov <- anova(out$optModel)    #  Extract a range of results 
   RSS <- tail(aov$"Sum Sq",1)
   df <- aov$Df
   nobs <- sum(df) + 1
   numfact <- length(df) - 1
   npars <- sum(df[1:numfact]) + 1
   AIC <- nobs * log(RSS/nobs) + (2 * npars)
   results[i,] <- c(outsum$adj.r.squared,NA,RSS,sum(aov$"Sum Sq") - RSS,npars,nobs,AIC)
}
results[2:nummod,"incR2"] <- results[2:nummod,"adjR2"]-results[1:(nummod-1),"adjR2"]
round(results,4)
round(trends,4)

```

By looking at the increments to the adjusted-R2 it is clear that the factor _DepCat_ has a larger impact on the variation accounted for than even _Vessel_. An earlier analysis, not shown here, used a different order of factors and that indicated that the order of factors within _labelM_ needed changing to that above so that the improvements in the adjusted-R2 proceeded sequentially. While the final optimum model has the same parameters and model fit, the advantage of re-ordering the factors in order of the variation they describe is that the change in trend of CPUE tends to converge more cleanly to a final trajectory.  The _AIC_ column identifies the optimum combination of factors with the smallest value indicating the optimum. Typically, if one plots each standardization on the same plot, while the later factors can be statistically significant, their effect upon the trajectory of the standardized CPUE can be minimal or appear to contribute mainly noise. Generally, the first three or four factors very often determine the final trend. If the standardization is to be used within a formal stock assessment it is the trend that matters so those final few factors may only have a minor effect. Even so, the objective here is to describe the trend not make predictions of any future trends, so the need to be strict about which parameters to include and which to exclude is less serious.

### Alternative Standardization Strategies

So far we have only considered General Linear Models (which with log-normal errors give the same results as simple linear models). If we wish to use alternative residual error structures then it would be necessary to use true GLMs (as in Generalized Linear Models). These would be necessary if, for example, there was a wish to attempt using perhaps a Gamma distribution instead of log-normal. In such a case we would need to use somewhat different syntax. The standard approach when using the Gamma distribution would be to use a log-link in the GLM. In such cases then the dependent variable would then be _CE_ rather than _LnCE_. The functions described so far are designed for use with log-normal residual errors that need a bias-correction. Gamma residual errors do not require such a bias-correction so we will need to work directly with the estimated coefficients. 


```{r gammaerrors, echo=TRUE}

labelM <- c("Year","Zone","Vessel","Month","DayNight","DepCat","Month:Zone")
sps1 <- makecategorical(labelM,sps)
mod <- makeonemodel(labelM,dependent="CE")

model4 <- glm(mod,family=Gamma(link="log"),data=sps1)
m4 <- summary(model4)$coefficients  # combine these with empty first year
yrval <- rbind(c(0,0,0,0),m4[grep("Year",rownames(m4)),])
# backtransform the gamma model parameter using a simple 'exp' function
# use the function scaleCE to rescale the results t a mean of 1.0 to
# ease comparison with other trends.
gamres <- cbind(yrval,exp(yrval[,"Estimate"]),scaleCE(exp(yrval[,"Estimate"])))
rownames(gamres) <- yrs # 2003:2014 from the chunk immediately above
round(gamres,4)

```


```{r compareCPUE, echo=TRUE,fig.width=7,fig.height=4.5,fig.align="center"}
plotstand(out,bars=TRUE)
lines(2003:2014,gamres[,6],lwd=2,col=4)
legend("bottomleft",legend=c("Geomean","Log-Normal","Gamma"),col=c(1,1,4),
       lty=c(2,1,1),bty="n",lwd=3)
```

__Figure 13.__ The standardization of the cpue data within the _sps_ data set comparing a LM using log-normal, with a GLM using Gamma residual errors. The dashed line is the geometric mean CPUE while the solid black line with 95% is the log-normal error standardization. Finally, the blue line is the Gamma error standardization. The points in each line all have mean values of 1.0.


### The Use of GAMs

Generalized Additive Models are an extension of GLMs in which at least some of the factors are replaced by fitting smooth surfaces to some of the factors that are considered to have a non-linear relationship with catch rates.

In order to run them, however, it is necessary to install a number of additional R packages. As an example, we could use a GAM to add a smoother to the Lat - Long data in the sps data set. We would actually use the sps1 data set as the remaining categorical factors are also included in the analysis. A possible work-flow might involve the following code.

```{r useGAM, echo=TRUE}
# install and call these R packages and their dependencies
library(nlme)
library(mgcv)
library(gamm4)
# note the use of gam rather than lm or glm (see the examples in ?gam for more
# details. 
modelGam <- gam(LnCE ~ s(Long,Lat) + Year + Zone + Vessel + Month + 
                   DayNight + DepCat, data = sps1)
anova(modelGam)

```

We should not be surprised that the _Zone_ factor is no longer significant. By including the Lat - Long surface including the _Zone_ factor becomes redundant so we should really repeat the analysis without _Zone_ included. 

```{r useGAMnoZone, echo=TRUE}
 
modelGam <- gam(LnCE ~ s(Long,Lat) + Year + Vessel + Month + 
                   DayNight + DepCat, data = sps1)
anova(modelGam)

```

We can use the _getfact_ function to extract the results we need. The Coeff column contains the LogCE transformed back to the linear scale and Scaled is the Coeff re-scaled to a mean of 1.0. Once again if it is desired to scale this to the nominal CPUE from the fishery so as to improve communication with Industry and managers then we can use _geomean_ to estimate the overall geometric mean to re-scale the 'Scaled' column to something more meaningful to industry members. 

```{r getresults, echo=TRUE}

answer <- getfact(modelGam,"Year")
opti <- answer[,"Scaled"]
round(answer,5)
```

We can gain an impression of the surface fitted to the Lat - Long data using the _plot_ function, which recognizes the output from a gam and can react accordingly.


```{r plotgam, fig.width=5,fig.height=8,fig.align="center"}
#plotprep(width=4.5,height=7)
 plot(modelGam,ylim=c(-44.5,-40),xlim=c(143.5,146.5),se=FALSE,xlab="",ylab="")
 title(ylab=list("Latitude", cex=1.0, font=7),
       xlab=list("Longitude", cex=1.0, font=7))
 plotLand("pink")

```

__Figure 14.__ A plot of the surface fitted to the output from the gam function.


The effect on the year parameters is what we are really interested in for the purposes of stock assessment and we can compare the outcome of the GAM with the previous GLM.



```{r gamvsglm, fig.width=7,fig.height=4.5,fig.align="center"}
#plotprep(width=7,height=4.5)
plotstand(out,bars=TRUE)
lines(facttonum(out$years),opti,col=4,lwd=2)
legend("bottomleft",c("GLM","GAM"),col=c(1,4),lwd=3,bty="n",cex=1.2)

```

__Figure 15__ A comparison of the standardization produced by the General Linear Model with that produced by the General Additive Model.

